## SETTING THE TRADING ENVIRONMENT

class StockTradingEnvironment(gym.Env):
    def __init__(self, data, window_size, trading_fee, initial_balance=1000000):
        super(StockTradingEnvironment, self).__init__()

        self.data = data
        self.window_size = window_size
        self.trading_fee = trading_fee
        self.initial_balance = initial_balance
        self.current_step = window_size

        # Action space: 0 = BUY, 1 = HOLD, 2 = SHORT
        self.action_space = spaces.Discrete(3)

        # Observation space:
        self.observation_space = spaces.Box(low=0, high=1, shape=(39,))

    def reset(self):
        self.current_step = self.window_size
        self.balance = self.initial_balance
        self.shares_held = 0
        self.net_worth = self.balance
        return self._get_observation()

    def step(self, action):
        self._take_action(action)
        self.current_step += 1

        if self.current_step == len(self.data) - 1:
            done = True
        else:
            done = False

        obs = self._get_observation()
        reward = self._calculate_reward()

        return obs, reward, done, {}

    def _get_observation(self):
        obs = [self.data[ALL_COL].iloc[self.current_step]]

        return np.array(obs)

    def _take_action(self, action):
        current_price = self.data['close'].iloc[self.current_step]

        if action == 0:  # BUY
            self.shares_held += (self.balance / current_price) * (1 - self.trading_fee)
            self.balance -= self.balance * self.trading_fee
        elif action == 1:  # HOLD
            pass
        elif action == 2:  # SHORT
            self.balance += self.shares_held * current_price * (1 - self.trading_fee)
            self.shares_held = 0

        self.net_worth = self.balance + (self.shares_held * current_price)

    def _calculate_reward(self):
        prev_net_worth = self.net_worth - self.balance
        current_net_worth = self._get_observation()[0] - self.balance
        return current_net_worth - prev_net_worth
     
SETTING THE ENVIRONMENT

# Initialize trading environment with real data
env_train = StockTradingEnvironment(train_data, window_size=5, trading_fee=0)
env_test = StockTradingEnvironment(test_data, window_size=5, trading_fee=0)

# DQN
# num_episodes = 5
batch_size = 32
state_size = env_train.observation_space.shape[0]
action_size = env_train.action_space.n

agent = DQNAgent(state_size, action_size)
