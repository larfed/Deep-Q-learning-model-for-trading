# SELECT THE STOCKS FOR THE EVALUATION AND EPISODE LENGHT FOR TRAINING


SELECTED_ISTRUMENTS = ['JPM','MSFT','JNJ']
num_episodes = 50
IMPORT DATA

############################################################
##cleaned.csv formation
############################################################

# Define a list of instruments (e.g., stocks or cryptocurrencies)
# also vix in this version

CONFIG_TARGET_INSTRUMENTS = [SELECTED_ISTRUMENTS[0]]

start_date = "2014-01-01"
end_date = "2024-01-01"

# Initialize an empty DataFrame to store the combined data
combined_data = pd.DataFrame()

# Iterate over each instrument
for instrument in CONFIG_TARGET_INSTRUMENTS:
    # Download historical data using yfinance
    historical_data = yf.download(instrument, start=start_date, end=end_date)
    historical_data['instrument'] = instrument
    vix_value = yf.download('^VIX',start=start_date, end=end_date)
    historical_data['VIX'] = vix_value['Close']

    # Reset the index to make 'Date' a column
    historical_data = historical_data.reset_index()
    #historical_data['Date'] = pd.to_datetime(historical_data['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d %H:%M:%S%z')
    historical_data['Date'] = pd.to_datetime(historical_data['Date'], format='%Y-%m-%d %H:%M:%S').dt.strftime('%Y-%m-%d 16:00:00%z')
    historical_data['Date'] = pd.to_datetime(historical_data['Date'], format='%Y-%m-%d %H:%M:%S%z').dt.tz_localize('US/Pacific')


    # lower case columns
    historical_data.columns = historical_data.columns.str.lower()

    ####################################################


    # Calculate the Typical Price
    typical_price = (historical_data['high'] + historical_data['low'] + historical_data['close']) / 3

    # Calculate the Traded Value (Typical Price * Volume)
    traded_value = typical_price * historical_data['volume']

    # Calculate Cumulative Traded Value
    cumulative_traded_value = traded_value.cumsum()

    # Calculate Cumulative Volume
    cumulative_volume = historical_data['volume'].cumsum()

    # Calculate VWAP
    vwap = cumulative_traded_value / cumulative_volume

    historical_data['weightedAverage'] = vwap
    ####################################################

    historical_data['quoteVolume'] = historical_data['volume'] * historical_data['weightedAverage']
    ####################################################

    # Merge the current instrument's data with the combined data
    if combined_data.empty:
        combined_data = historical_data
    else:
        combined_data=pd.concat([combined_data,historical_data], axis=0, ignore_index=True)

# Sort the combined data by date
combined_data = combined_data.sort_values(by='date')

# Reset index for the final combined data
combined_data = combined_data.reset_index(drop=True)

# Define the desired order of columns
desired_order = ['date', 'instrument', 'high', 'low', 'open', 'close', 'volume', 'quoteVolume','weightedAverage', 'vix']
combined_data = combined_data[desired_order]


####################################################
combined_data = combined_data.dropna()
####################################################
#rounding to 8 decimals
combined_data = combined_data.round(8)
#combined_data['date'] = pd.to_datetime(combined_data['date'])

combined_data.to_csv('cleaned.csv', index=False)
####################################################
combined_data = combined_data.drop('instrument',axis=1)
# Display the result
combined_data

############################################################
##cleaned_preprocessed.csv formation
############################################################
import pandas as pd
FILE = "cleaned.csv"
df = pd.read_csv(FILE)

INSTRUMENTS = CONFIG_TARGET_INSTRUMENTS
############################################################

COLS = ['high', 'low', 'open', 'close', 'volume', 'quoteVolume','weightedAverage','vix']
SCOLS = ["vh","vl","vc","open_s","volume_s","quoteVolume_s","weightedAverage_s",'vix']
OBS_COLS = ['vh', 'vl', 'vc', 'open_s', 'volume_s', 'quoteVolume_s', 'weightedAverage_s', 'vh_roll_7', 'vh_roll_14', 'vh_roll_30', 'vl_roll_7', 'vl_roll_14', 'vl_roll_30', 'vc_roll_7', 'vc_roll_14', 'vc_roll_30', 'open_s_roll_7', 'open_s_roll_14', 'open_s_roll_30', 'volume_s_roll_7', 'volume_s_roll_14', 'volume_s_roll_30', 'quoteVolume_s_roll_7', 'quoteVolume_s_roll_14', 'quoteVolume_s_roll_30', 'weightedAverage_s_roll_7', 'weightedAverage_s_roll_14', 'weightedAverage_s_roll_30', 'vix_roll_7', \
                'vix_roll_14', 'vix_roll_30']
EPISODE_LENGTH = 500


df["date"] = df["date"].apply(lambda x: pd.Timestamp(x, unit='s', tz='US/Pacific'))
df = df[df["instrument"].isin(INSTRUMENTS)].sort_values("date")
df["vh"] = df["high"]/df["open"]
df["vl"] = df["low"]/df["open"]
df["vc"] = df["close"]/df["open"]
df["open_s"] = df.groupby("instrument")["open"].apply(lambda x: x - x.shift(1))
df["volume_s"] = df.groupby("instrument")["volume"].apply(lambda x: x - x.shift(1))
df["quoteVolume_s"] = df.groupby("instrument")["quoteVolume"].apply(lambda x: x - x.shift(1))
df["weightedAverage_s"] = df.groupby("instrument")["weightedAverage"].apply(lambda x: x - x.shift(1))
df["vix"] = df.groupby("instrument")["vix"].apply(lambda x: x - x.shift(1))

new_cols = []

for col in SCOLS:
    print(col)
    df[col+"_roll_7"] = df.groupby("instrument")[col].apply(lambda x: x.rolling(7).mean().bfill())
    new_cols.append(col+"_roll_7")
    df[col+"_roll_14"] = df.groupby("instrument")[col].apply(lambda x: x.rolling(14).mean().bfill())
    new_cols.append(col+"_roll_14")
    df[col+"_roll_30"] = df.groupby("instrument")[col].apply(lambda x: x.rolling(30).mean().bfill())
    new_cols.append(col+"_roll_30")

SCOLS.extend(new_cols)
print(SCOLS)
###############################
df = df.dropna()
df = df.reset_index(drop=True)
cut_off_date = pd.Timestamp("2023-01-01", tz='US/Pacific') ## CUT DATE FOR TRAINING AND TEST DATA

# Filter data based on the cut-off date
train_data = df[df['date'] < cut_off_date]
test_data = df[df['date'] >= cut_off_date]
